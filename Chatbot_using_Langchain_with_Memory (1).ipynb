{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_YvtM_zXz_MY",
        "outputId": "fe213579-76a1-4cf3-afe1-b3adaf7c0b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.16-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.40)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.10.6)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.24.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.25.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (0.3.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.68.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain_google_genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.11-py3-none-any.whl (39 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.16-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.16 langchain_google_genai-2.0.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "dc98db3fe75140d2b9843172dede91a6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain"
      ],
      "metadata": {
        "id": "2FrvcJkl0cyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrGGIzgW0ov-",
        "outputId": "b10c2f5e-3221-4620-be70-cca7774c3434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "0B8Ytfxm1O-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GEMINI_API_KEY\"] = \"your token\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"your token\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Chatbot\"\n",
        "os.environ[\"LANGCHAIN_END_POINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"your token\"\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"your token\""
      ],
      "metadata": {
        "id": "l25fCc_B0uhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "H6CWe73F15IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",temperature=0,api_key=os.getenv(\"GEMINI_API_KEY\"))"
      ],
      "metadata": {
        "id": "wVvyYCIh2IS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAUMLen92WB8",
        "outputId": "53b9d36e-8f93-4972-f3ac-49b182b83db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-b1c03165-ebf3-40ca-a438-9ee7a230b121-0', usage_metadata={'input_tokens': 1, 'output_tokens': 11, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "CYlwviMA44pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "xlrWwYD75MCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser.invoke(model.invoke(\"hi\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sn4vqf1y5U3R",
        "outputId": "4b8a8f86-944c-48eb-d565-ccd9500e6ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi there! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "BQA1bdn05iYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    message = input(\"Write your Query\")\n",
        "    if message == \"bye\":\n",
        "      print(\"Good bye, have a great day\")\n",
        "      break\n",
        "    else:\n",
        "      print(parser.invoke(model.invoke([HumanMessage(content=message)])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAJ7l-aV5yYZ",
        "outputId": "08b26c20-742a-40e5-8a75-e7e7ffaa3d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write your Querybye\n",
            "Good bye, have a great day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cvdvsxlJCC6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import BaseChatMessageHistory"
      ],
      "metadata": {
        "id": "AycZQV9V84Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory"
      ],
      "metadata": {
        "id": "eNWUd8gOBsPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "NMuYQV1IBuE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "oh_17bMbClKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ff0b25-882c-4d72-be01-69f352069d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import  AIMessage"
      ],
      "metadata": {
        "id": "NMwxIOopBvqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! i am ronaldo\"),\n",
        "        AIMessage(content=\"hello ronaldo. how can i assist you today ?\"),\n",
        "        HumanMessage(content=\"What is my name ?\")\n",
        "\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "TDUZRopZCi9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser.invoke(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2M1oQJACDBEZ",
        "outputId": "c757a1cd-a682-425b-9c63-a906735cfa5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As a large language model, I don't know your name. You told me your name is Ronaldo, but I have no way to verify that or remember it beyond this conversation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store={}"
      ],
      "metadata": {
        "id": "rXR8l7A-DC-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]"
      ],
      "metadata": {
        "id": "JrJKGJeeE2Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"firstchat\"}}"
      ],
      "metadata": {
        "id": "DU2s9FodE92N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_memory=RunnableWithMessageHistory(model,get_session_history)"
      ],
      "metadata": {
        "id": "A_B6doOfFBpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_memory.invoke([HumanMessage(content=\"Hi! I'm viren\")],config=config).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K_Gjt9bXCbXY",
        "outputId": "e253c685-60ac-4843-fe99-137bad591e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi Viren! It's nice to meet you. How can I help you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_memory.invoke([HumanMessage(content=\"what is my name ?\")],config=config).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mgZtexVqCcNd",
        "outputId": "8ff27808-6a80-43d6-9945-3cfc5fd050fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Viren.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"secondtchat\"}}"
      ],
      "metadata": {
        "id": "2qK6j--eDFsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_memory.invoke([HumanMessage(content=\"what is my name?\")],config=config).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-yGtl476DcGT",
        "outputId": "a7a75d76-ec25-48e7-8ec1-8221a8b87818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As an AI, I don't know your name. You haven't told me!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHrN4myfDeJO",
        "outputId": "fc6cad80-d1b4-4057-db5b-95adb64a5420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'firstchat': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"Hi! I'm sunnysavita\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi sunnysavita! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-dda75e51-21cb-4291-bdb8-e6157cd1424c-0', usage_metadata={'input_tokens': 9, 'output_tokens': 22, 'total_tokens': 31, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=\"Hi! I'm viren\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Viren! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-714ac9aa-0954-4c63-9d9d-297115bbca9a-0', usage_metadata={'input_tokens': 37, 'output_tokens': 20, 'total_tokens': 57, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='what is my name ?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Viren.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e3528991-2e88-449a-8ca7-6dd0e181592f-0', usage_metadata={'input_tokens': 61, 'output_tokens': 7, 'total_tokens': 68, 'input_token_details': {'cache_read': 0}})]),\n",
              " 'secondtchat': InMemoryChatMessageHistory(messages=[HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I don't know your name. You haven't told me!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-5e9b30ba-2397-4cbc-8e8e-27b6ef2fb28a-0', usage_metadata={'input_tokens': 5, 'output_tokens': 20, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}})])}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
      ],
      "metadata": {
        "id": "Wduhii9CDk3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "      (\"system\",\"You are a helpful assistant. Answer all questions to the best of your ability.\",),\n",
        "      MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "V1RYuY9iEOI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model"
      ],
      "metadata": {
        "id": "mu6cZshyERtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"messages\":[\"Can you tell what is RAG?\",\"Why RAG is useful?\",\"Can you tell how one can learn RAG ?\"]}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uEpWo8h1E6CM",
        "outputId": "1da88968-babe-4b0a-f1f5-010844c75e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Okay, let\\'s break down RAG:\\n\\n**What is RAG?**\\n\\nRAG stands for **Retrieval-Augmented Generation**. It\\'s an AI framework that enhances the capabilities of large language models (LLMs) like GPT-3, GPT-4, or similar models.  Instead of *solely* relying on the knowledge embedded within the LLM\\'s parameters (which was learned during its training), RAG allows the LLM to access and incorporate information from an *external* knowledge source *before* generating a response.\\n\\nHere\\'s a simplified breakdown of the process:\\n\\n1.  **Retrieval:** When a user asks a question, the RAG system first *retrieves* relevant information from an external knowledge base (this could be a document library, a database, a website, etc.).  This retrieval is typically done using techniques like semantic search or keyword search to find the most relevant pieces of information.\\n\\n2.  **Augmentation:** The retrieved information is then *augmented* (added) to the original user query.  This combined information (the original question + the retrieved context) is then fed into the LLM.\\n\\n3.  **Generation:** The LLM uses *both* its pre-existing knowledge *and* the newly provided context to generate a more informed and accurate response.\\n\\n**Why is RAG Useful?**\\n\\nRAG offers several key advantages:\\n\\n*   **Improved Accuracy and Reduced Hallucinations:** LLMs can sometimes \"hallucinate\" or make up information, especially when asked about topics outside their training data. By grounding the LLM in external knowledge, RAG significantly reduces the likelihood of these hallucinations and improves the accuracy of the responses.\\n\\n*   **Access to Up-to-Date Information:** LLMs have a knowledge cutoff date (the date up to which they were trained). RAG allows the LLM to access and incorporate the latest information from external sources, making it possible to answer questions about recent events or rapidly changing topics.\\n\\n*   **Domain-Specific Knowledge:**  LLMs are trained on vast amounts of general data. RAG enables you to tailor the LLM to specific domains or industries by providing it with access to relevant domain-specific knowledge bases.  For example, you could use RAG to build a chatbot that can answer questions about legal documents, medical records, or financial reports.\\n\\n*   **Explainability and Traceability:** Because the LLM\\'s response is based on specific retrieved documents, it\\'s easier to understand *why* the LLM provided a particular answer. You can often trace the response back to the source documents, which increases trust and transparency.\\n\\n*   **Customization and Control:** RAG gives you more control over the information that the LLM uses to generate responses. You can curate and update the knowledge base to ensure that the LLM is using the most accurate and relevant information.\\n\\n**How to Learn RAG**\\n\\nLearning RAG involves understanding several different areas. Here\\'s a suggested path:\\n\\n1.  **Foundational Knowledge:**\\n\\n    *   **Large Language Models (LLMs):**  Start with a basic understanding of how LLMs work, their strengths, and their limitations.  Resources:\\n        *   Read blog posts and articles about LLMs (e.g., on the OpenAI website, Hugging Face blog, etc.).\\n        *   Take introductory courses on natural language processing (NLP) and deep learning.  Coursera, edX, and Udacity offer excellent options.\\n    *   **Vector Databases:**  Learn about vector databases (like Pinecone, Weaviate, Chroma, Milvus, FAISS).  These databases are used to store and efficiently search for semantically similar information.  Understand concepts like vector embeddings, similarity search, and indexing.\\n    *   **Embeddings:** Understand how text is converted into numerical vectors (embeddings) using models like OpenAI\\'s `text-embedding-ada-002`, or open-source models from Hugging Face.\\n\\n2.  **Core RAG Concepts and Techniques:**\\n\\n    *   **Document Loading and Preprocessing:** Learn how to load documents from various sources (PDFs, websites, databases) and preprocess them for RAG.  This includes tasks like text cleaning, splitting documents into chunks, and removing irrelevant information.\\n    *   **Indexing:** Understand how to create an index of your knowledge base in a vector database.  This involves generating embeddings for each chunk of text and storing them in the database.\\n    *   **Retrieval Strategies:** Explore different retrieval techniques, such as:\\n        *   **Semantic Search:** Using vector embeddings to find documents that are semantically similar to the user\\'s query.\\n        *   **Keyword Search:** Using traditional keyword-based search methods (e.g., TF-IDF, BM25) to find relevant documents.\\n        *   **Hybrid Search:** Combining semantic search and keyword search for improved results.\\n        *   **Metadata Filtering:** Filtering the search results based on metadata associated with the documents (e.g., date, author, category).\\n    *   **Query Transformation:** Learn how to rephrase or expand the user\\'s query to improve retrieval accuracy.  This might involve techniques like query expansion, query rewriting, or query decomposition.\\n    *   **Response Generation:** Understand how to combine the retrieved information with the user\\'s query to generate a coherent and informative response using the LLM.  This might involve techniques like prompt engineering, context injection, and response ranking.\\n\\n3.  **Tools and Libraries:**\\n\\n    *   **LangChain:** A popular framework for building LLM-powered applications, including RAG systems.  It provides modules for document loading, text splitting, vector store integration, retrieval, and response generation.  Start with the LangChain documentation and tutorials.\\n    *   **LlamaIndex (GPT Index):** Another powerful framework specifically designed for building RAG applications.  It offers similar functionality to LangChain, with a focus on indexing and querying data.\\n    *   **Hugging Face Transformers:** A library for working with pre-trained language models, including models for generating embeddings.\\n    *   **Vector Databases:**  Get hands-on experience with at least one vector database (e.g., Pinecone, Weaviate, Chroma).\\n\\n4.  **Hands-on Projects:**\\n\\n    *   **Build a Simple RAG System:** Start with a small project, such as building a RAG system that can answer questions about a single document or a small set of documents.\\n    *   **Experiment with Different Retrieval Strategies:** Try different retrieval techniques (semantic search, keyword search, hybrid search) and compare their performance.\\n    *   **Evaluate RAG Performance:**  Learn how to evaluate the performance of your RAG system using metrics like accuracy, relevance, and coherence.\\n    *   **Contribute to Open Source Projects:**  Contribute to open-source RAG projects to gain experience and learn from other developers.\\n\\n5.  **Resources:**\\n\\n    *   **LangChain Documentation:** [https://www.langchain.com/](https://www.langchain.com/)\\n    *   **LlamaIndex Documentation:** [https://www.llamaindex.ai/](https://www.llamaindex.ai/)\\n    *   **Hugging Face:** [https://huggingface.co/](https://huggingface.co/)\\n    *   **Vector Database Documentation:** (Pinecone, Weaviate, Chroma, etc.)\\n    *   **Research Papers:**  Read research papers on RAG and related topics to stay up-to-date with the latest advancements.\\n    *   **Online Courses and Tutorials:** Look for online courses and tutorials on RAG on platforms like Coursera, Udemy, and YouTube.\\n\\n**Key Skills to Develop:**\\n\\n*   **Python Programming:**  Essential for working with RAG frameworks and libraries.\\n*   **Natural Language Processing (NLP):**  Understanding NLP concepts and techniques is crucial for building effective RAG systems.\\n*   **Data Engineering:**  Skills in data loading, preprocessing, and storage are important for managing the knowledge base.\\n*   **Machine Learning (ML):**  A basic understanding of machine learning concepts is helpful for understanding how LLMs and embedding models work.\\n*   **Prompt Engineering:**  The ability to craft effective prompts for LLMs is essential for generating high-quality responses.\\n\\nBy following these steps and dedicating time to learning and experimentation, you can develop a strong understanding of RAG and its applications. Good luck!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_memory=RunnableWithMessageHistory(chain,get_session_history)"
      ],
      "metadata": {
        "id": "aMe-F9kjFFes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"thirdchat\"}}"
      ],
      "metadata": {
        "id": "ms2z4s63IEk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=model_with_memory.invoke(\n",
        "    [HumanMessage(content=\"Hi! I'm Jim\"),\n",
        "     ],config=config\n",
        ")"
      ],
      "metadata": {
        "id": "7zFtQ4lqIGe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CjnqVL2qIJcc",
        "outputId": "5f00b9b6-0a5b-4eeb-feec-5c9276e327c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi Jim! It's nice to meet you. How can I help you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=model_with_memory.invoke(\n",
        "    [HumanMessage(content=\"what is my name?\"),\n",
        "     ],config=config\n",
        ")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQqN_FKAILwL",
        "outputId": "3a4c1c8c-e3f3-40a1-dc9d-2f856a424f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Jim.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPCDOaa9IW5m",
        "outputId": "8bd317ff-2795-4992-935c-097b3e91281c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'firstchat': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"Hi! I'm sunnysavita\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi sunnysavita! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-dda75e51-21cb-4291-bdb8-e6157cd1424c-0', usage_metadata={'input_tokens': 9, 'output_tokens': 22, 'total_tokens': 31, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=\"Hi! I'm viren\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Viren! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-714ac9aa-0954-4c63-9d9d-297115bbca9a-0', usage_metadata={'input_tokens': 37, 'output_tokens': 20, 'total_tokens': 57, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='what is my name ?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Viren.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e3528991-2e88-449a-8ca7-6dd0e181592f-0', usage_metadata={'input_tokens': 61, 'output_tokens': 7, 'total_tokens': 68, 'input_token_details': {'cache_read': 0}})]),\n",
              " 'secondtchat': InMemoryChatMessageHistory(messages=[HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I don't know your name. You haven't told me!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-5e9b30ba-2397-4cbc-8e8e-27b6ef2fb28a-0', usage_metadata={'input_tokens': 5, 'output_tokens': 20, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}})]),\n",
              " 'thirdchat': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"Hi! I'm Jim\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Jim! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-2141e4b7-1e1d-433e-96db-dbfe449bca40-0', usage_metadata={'input_tokens': 22, 'output_tokens': 19, 'total_tokens': 41, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Jim.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-c1609131-fe42-4224-89fb-2d8276568d56-0', usage_metadata={'input_tokens': 45, 'output_tokens': 6, 'total_tokens': 51, 'input_token_details': {'cache_read': 0}})])}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trimming the messages"
      ],
      "metadata": {
        "id": "xdcceSdeKfu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, trim_messages"
      ],
      "metadata": {
        "id": "-xtR7281IY4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trimmer = trim_messages(\n",
        "    max_tokens=40,\n",
        "    strategy=\"last\",\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\",\n",
        ")"
      ],
      "metadata": {
        "id": "UbOv2ZpDKkES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    HumanMessage(content=\"hi! I'm bob\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"whats 2 + 2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem!\"),\n",
        "    HumanMessage(content=\"having fun?\"),\n",
        "    AIMessage(content=\"yes!\"),\n",
        "]"
      ],
      "metadata": {
        "id": "NVPl9loRKnpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_num_tokens_from_messages(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fE23i4wKzhV",
        "outputId": "a3b91b88-683a-4573-db07-28fe3d6ba148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trimmer.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq9w78XJK3pM",
        "outputId": "fdc0e7ff-fb70-4a21-c967-b0b5bf3bdbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trimmed_message = trimmer.invoke(messages)"
      ],
      "metadata": {
        "id": "pEitwuXnLbeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_num_tokens_from_messages(trimmed_message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5HLEZcJLhP7",
        "outputId": "6654b025-e800-45c3-8a2c-51bc3b8c1212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
        "    | prompt\n",
        "    | model\n",
        ")\n",
        "\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
        "        \"language\": \"English\",\n",
        "    }\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mYQ8U8S7LjLj",
        "outputId": "b19306a7-f67e-469b-ec3d-508264ee1042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As a large language model, I have no memory of past conversations. Therefore, I don't know your name. You haven't told me!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"fifthchat\"}}"
      ],
      "metadata": {
        "id": "WfJlslqQMZUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_with_memory = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\",\n",
        ")"
      ],
      "metadata": {
        "id": "WTZYO0eXM6zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model_with_memory.invoke(\n",
        "    {\n",
        "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
        "        \"language\": \"English\",\n",
        "    },\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "laQJdthGMyDI",
        "outputId": "1b27c46f-ebf9-432f-b61a-b5e579c1f252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As a large language model, I have no memory of past conversations. Therefore, I don't know your name. You haven't told me!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model_with_memory.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
        "        \"language\": \"English\",\n",
        "    },\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "saN0bAMOM3KG",
        "outputId": "880f3e12-7837-43f7-e7e4-25d45a613ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, but as a language model, I have no memory of past conversations. Therefore, I don't know what math problem you asked previously. If you'd like to ask it again, I'm ready to help!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: i want to push this colab file in my github . Can you give a readme file which describes this project ?\n",
        "\n",
        "# LangChain Google Gemini Chatbot\n",
        "\n",
        "This Colab notebook demonstrates a chatbot built using LangChain and Google Gemini.  It showcases several key features of LangChain, including:\n",
        "\n",
        "* **Google Gemini Integration:** Utilizes the Gemini API for natural language processing.\n",
        "* **Message History Management:** Implements chat history using `InMemoryChatMessageHistory`, demonstrating how to maintain context across multiple user interactions.\n",
        "* **Customizable Sessions:** Allows for multiple chat sessions with separate histories.\n",
        "* **Prompt Templates:**  Uses `ChatPromptTemplate` for structured prompts, enhancing interaction quality.\n",
        "* **Message Trimming:** Demonstrates message trimming to manage context window limits, ensuring efficient token usage.\n",
        "* **Runnable Chains:**  Combines components (prompt template, model, and message history) into a streamlined `Runnable` for simplified interaction flow.\n",
        "\n",
        "\n",
        "## Setup and Dependencies\n",
        "\n",
        "The notebook installs necessary libraries:\n",
        "\n",
        "* `langchain_google_genai`\n",
        "* `langchain`\n",
        "* `langchain_community`\n",
        "\n",
        "It also sets environment variables for API keys and LangChain tracing, which are crucial for running the chatbot. **Remember to replace the placeholder API keys with your actual keys.**\n",
        "\n",
        "## Usage\n",
        "\n",
        "The chatbot starts with an initial greeting and enters a loop, prompting the user for input until the user types \"bye\".\n",
        "\n",
        "The notebook then provides examples of different functionalities:\n",
        "\n",
        "1. **Basic Interaction:** Demonstrates a simple interaction with the Gemini model.\n",
        "\n",
        "2. **Message History:** Shows how to use `InMemoryChatMessageHistory` to store and retrieve chat history, thus keeping track of previous interactions within a conversation.\n",
        "\n",
        "3. **Multiple Sessions:** Highlights how to create and manage multiple independent chat sessions, each with its own history.\n",
        "\n",
        "4. **Prompt Templates:** Explains the use of prompt templates for formatting messages to the Gemini model, leading to more coherent responses.\n",
        "\n",
        "5. **Message Trimming:**  Demonstrates how to limit the number of tokens used in the context window using `trim_messages`. This is important to avoid exceeding token limits.\n",
        "\n",
        "6. **Runnable Chains with Memory:** Shows how to create a runnable chain that includes the message history, enabling the model to use the current chat session data.\n",
        "\n",
        "\n",
        "## API Keys and Environment Variables\n",
        "\n",
        "Ensure that you have set the following environment variables before running the notebook:\n",
        "\n",
        "*   `GEMINI_API_KEY`: Your Google Gemini API key.\n",
        "*   `LANGCHAIN_API_KEY`: Your LangChain API key.\n",
        "*   `LANGCHAIN_TRACING_V2`, `LANGCHAIN_PROJECT`, `LANGCHAIN_END_POINT`: Required for LangChain tracing.\n",
        "*   `TAVILY_API_KEY`:  Your Tavily API key (if used).\n",
        "*   `HUGGINGFACEHUB_API_TOKEN`: Your Hugging Face API key (if used).\n",
        "\n",
        "**Important:**  Replace the placeholder values with your actual API keys.\n",
        "\n",
        "## Running the Notebook\n",
        "\n",
        "1.  Open the notebook in Google Colab.\n",
        "2.  Replace the placeholder API keys with your own.\n",
        "3.  Run all the cells.\n",
        "\n",
        "You'll be able to interact with the chatbot and observe how the different LangChain features work.\n",
        "\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Feel free to enhance the code, add more functionalities, or improve the documentation.\n"
      ],
      "metadata": {
        "id": "q9s5R_nwNAYG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}